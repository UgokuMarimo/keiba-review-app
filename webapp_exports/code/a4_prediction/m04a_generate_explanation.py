# C:\KeibaAI\code\a4_prediction\m04a_generate_explanation.py (SHAPå€¤ãã®ã¾ã¾å…¥åŠ› & æ•°å€¤â†’æ–‡å­—å¤‰æ›å¯¾å¿œ)
'''
google ai studioã®APIã‚­ãƒ¼ã‚’è¨­å®šå¾Œå®Ÿè¡Œå¯èƒ½
 $env:GOOGLE_API_KEY = 'APIã‚­ãƒ¼' 

python code/a4_prediction/m04a_generate_explanation.py [ãƒ¬ãƒ¼ã‚¹ID] [ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—] [äºˆæ¸¬é †ä½ä½•ä½ã®é¦¬ã®è¨€èªåŒ–ã™ã‚‹ã®ã‹] 

python code/a4_prediction/m04a_generate_explanation.py 202508040411 B 1
'''



import os
import sys
import json
import argparse
import pandas as pd

# --- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¨­å®š ---
_current_dir = os.path.dirname(os.path.abspath(__file__)); PROJECT_ROOT = os.path.abspath(os.path.join(_current_dir, '..', '..')); sys.path.append(PROJECT_ROOT); sys.path.append(os.path.join(PROJECT_ROOT, 'code'))

# --- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# explanation_templatesã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from explanation_templates import FEATURE_GROUPS, get_group_for_feature, get_original_value_display, EXPLANATION_TEMPLATES
from utils.llm_utils import generate_text_with_gemini

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: SHAPãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å®šã®ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®å€¤ã‚’å–å¾— (å¤‰æ›´ãªã—) ---
def _get_factor_value(shap_df: pd.DataFrame, feature_name: str):
    """shap_dfã‹ã‚‰ç‰¹å®šã®ç‰¹å¾´é‡ã®å€¤ã‚’å–å¾—ã™ã‚‹"""
    if feature_name in shap_df['feature'].values:
        return shap_df[shap_df['feature'] == feature_name]['value'].iloc[0]
    return None

def generate_prompt(shap_data: dict, shap_df: pd.DataFrame) -> str:
    """SHAPãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰LLMã«ä¸ãˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""

    # 1. ç‰¹å¾´é‡ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è²¢çŒ®åº¦ã‚’åˆç®—
    shap_df['group'] = shap_df.apply(lambda row: get_group_for_feature(row['feature'], row['shap_value']), axis=1)
    group_summary = shap_df.groupby('group')['shap_value'].sum().sort_values(ascending=False)

    positive_themes_list = []
    negative_themes_list = []

    for group_name, total_shap in group_summary.items():
        if abs(total_shap) < 0.05: continue # å½±éŸ¿ã®å°ã•ã„ã‚°ãƒ«ãƒ¼ãƒ—ã¯ç„¡è¦–

        theme_body = f"\n## ãƒ†ãƒ¼ãƒï¼š{group_name} (ç·åˆè²¢çŒ®åº¦: {total_shap:+.2f})\n"
        
        # ã“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯è¤‡åˆãƒ«ãƒ¼ãƒ«ã‚„å˜ä¸€ç‰¹å¾´é‡ã®ç‰¹å®šã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ä½¿ç”¨ã›ãšã€
        # LLMã«ç›´æ¥SHAPãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã€è‡ªç”±ãªè§£é‡ˆã‚’ä¿ƒã—ã¾ã™ã€‚
        group_features = shap_df[shap_df['group'] == group_name].sort_values('shap_value', ascending=False)

        # å€‹ã€…ã®ç‰¹å¾´é‡ã®å†…è¨³ã‚’è¨˜è¿°
        for _, factor in group_features.iterrows():
            feature_name = factor['feature']
            numeric_value = factor['value']
            shap_value = factor['shap_value']

            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®æ–‡å­—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
            value_display = get_original_value_display(feature_name, numeric_value)

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã€ç‰¹å¾´é‡åã€å¤‰æ›å¾Œã®å€¤ã€SHAPå€¤ã‚’ãã®ã¾ã¾æ¸¡ã™
            if shap_value >= 0:
                reason_text = EXPLANATION_TEMPLATES["default_positive"](feature_name, value_display, shap_value)
            else:
                reason_text = EXPLANATION_TEMPLATES["default_negative"](feature_name, value_display, shap_value)

            theme_body += f"- {reason_text}\n"
        
        if total_shap > 0:
            positive_themes_list.append(theme_body)
        else:
            negative_themes_list.append(theme_body)

    # 3. æœ€çµ‚çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦ã‚‹
    prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®ç«¶é¦¬äºˆæƒ³å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸAIã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€ç«¶èµ°é¦¬ã€Œ{shap_data['horse_name']}ã€ã®è©•ä¾¡ã«ã¤ã„ã¦ã€ãƒ—ãƒ­ã®è¦–ç‚¹ã‹ã‚‰ã€Œãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„è§£èª¬æ–‡ã‚’200å­—ã‹ã‚‰300å­—ç¨‹åº¦ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

# AIã®ç·åˆè©•ä¾¡
- äºˆæ¸¬é †ä½: {shap_data['pred_rank']}ä½
- äºˆæ¸¬å€¤: {shap_data['pred_win_prob']:.4f}


# åˆ†æãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
--- ãƒã‚¸ãƒ†ã‚£ãƒ–è¦å›  ---
{chr(10).join(positive_themes_list) if positive_themes_list else "ç‰¹ã«ãªã—"}

--- ãƒã‚¬ãƒ†ã‚£ãƒ–è¦å›  ---
{chr(10).join(negative_themes_list) if negative_themes_list else "ç‰¹ã«ãªã—"}

# æŒ‡ç¤º
- ä¸Šè¨˜ã®ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã€ã«è¨˜è¼‰ã•ã‚ŒãŸå€‹ã€…ã®è¦å› ã‚’çµ±åˆã—ã€å˜ãªã‚‹ç®‡æ¡æ›¸ãã®è¦ç´„ã§ã¯ãªãã€ä¸€ã¤ã®æµæš¢ãªæ–‡ç« ã¨ã—ã¦ãã ã•ã„ã€‚
- ã€Œç·åˆè²¢çŒ®åº¦ã€ãŒå¤§ãã„ï¼ˆçµ¶å¯¾å€¤ï¼‰ãƒ†ãƒ¼ãƒã‚„ã€å€‹ã€…ã®è¦å› ã®è²¢çŒ®åº¦ãŒç‰¹ã«é«˜ã„ã‚‚ã®ã‚’ã€ã“ã®é¦¬ã‚’è©•ä¾¡ã™ã‚‹ä¸Šã§ã®é‡è¦ãªç†ç”±ã¨ã—ã¦è§£èª¬ã«å«ã‚ã¦ãã ã•ã„ã€‚
- ãƒ—ãƒ©ã‚¹è©•ä¾¡ã¨ãƒã‚¤ãƒŠã‚¹è©•ä¾¡ã®ãƒ†ãƒ¼ãƒã‚’æ¯”è¼ƒã—ã€ç·åˆçš„ã«ã©ã¡ã‚‰ãŒä¸Šå›ã£ã¦ã„ã‚‹ã‹ã‚’çµè«–ä»˜ã‘ã¦ãã ã•ã„ã€‚
- å°‚é–€ç”¨èªã¯é¿ã‘ãšã€ã—ã‹ã—ç«¶é¦¬åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚‹ã‚ˆã†ã«è£œè¶³ã—ãªãŒã‚‰è§£èª¬ã—ã¦ãã ã•ã„ã€‚
- ä¸å¯§èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ã€Œä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã§ã¯ã€ï½ã¨è¦‹ã‚‰ã‚Œã¾ã™ã€‚ã€ã®ã‚ˆã†ãªå½¢ã§çµè«–ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
- å‹ã¤ç¢ºç‡ã®ä½ã„é¦¬ã«é–¢ã—ã¦ç„¡ç†ã«é«˜ãè©•ä¾¡ã›ãšã€Œä»Šå›ã¯å³ã—ã„ã€ãªã©æ­£å½“ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€è©•ä¾¡ãŒä½ãã¦ã‚‚ã©ã®ã‚ˆã†ã«æµã¾ã‚Œã‚Œã°å‹ã¦ã‚‹ã‹ã‚‚ã¨è€ƒãˆã‚‹ã“ã¨ã®ã§ãã‚‹é¦¬ãŒãã®ã“ã¨ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„
"""
    return prompt

def main(race_id: str, model_type: str, rank: int): # model_typeã¯ä½¿ã‚ãªã„ãŒå¼•æ•°ã¯æ®‹ã—ã¦ãŠã
    print(f"--- [START] Generating explanation for race {race_id}, rank {rank} ---") # ã“ã“ã‚‚ä¿®æ­£ã—ã¾ã™
    shap_file_path = os.path.join(PROJECT_ROOT, 'shap_results', race_id, f"shap_rank_{rank}.json") # rank å¤‰æ•°ã‚’ä½¿ç”¨
    if not os.path.exists(shap_file_path): 
        print(f"[ERROR] SHAP result file not found: {shap_file_path}"); 
        return
    
    with open(shap_file_path, 'r', encoding='utf-8') as f: 
        shap_data = json.load(f)
    
    all_factors = shap_data.get('positive_factors', []) + shap_data.get('negative_factors', [])
    if not all_factors: 
        print("[ERROR] No SHAP factors found in the JSON file."); 
        return
    
    shap_df = pd.DataFrame(all_factors)
    
    print("\n--- Generating prompt for LLM... ---")
    prompt = generate_prompt(shap_data, shap_df)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤
    # print("\n--- Generated Prompt ---")
    # print(prompt)
    # print("--- End Prompt ---")

    print("\n--- Calling LLM API to generate explanation... ---")
    explanation_text = generate_text_with_gemini(prompt)
    
    print("\n" + "="*70)
    print(f"ğŸ´ {shap_data['horse_name']} (äºˆæ¸¬{shap_data['pred_rank']}ä½) ã®AIè§£èª¬")
    print("="*70)
    print(explanation_text)
    print("="*70)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate natural language explanation from SHAP results.")
    parser.add_argument('race_id', help='Target 12-digit race ID')
    parser.add_argument('model_type', help="Model type to use (e.g., 'B' or 'C')") # ä½¿ã‚ãªã„ãŒå¼•æ•°ã¯ç¶­æŒ
    #å‰Šé™¤ã—ã¦ã¿ã‚‹
    # parser.add_argument('run_shap', type=bool, nargs='?', default=True, help="Dummy arg to match predict.py signature.")
    parser.add_argument('rank', type=int, nargs='?', default=1, help="Target prediction rank to explain. Defaults to 1.")
    args = parser.parse_args()

    main(args.race_id, args.model_type.upper(), args.rank)